{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "70e7efd8",
      "metadata": {
        "id": "70e7efd8"
      },
      "source": [
        "# ZAF043_BA_Crowd-Anomaly-Detection "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15433a5e",
      "metadata": {
        "id": "15433a5e"
      },
      "source": [
        "## Overview\n",
        "Crowd anomaly detection is a type of machine learning model that is designed to detect unusual behavior in crowds of people. This can be useful for a variety of applications, including security, public safety, and crowd management.\n",
        "\n",
        "The basic idea behind a crowd anomaly detection model is to use machine learning algorithms to analyze video footage or other types of sensor data in order to identify patterns of behavior that are outside the norm. These patterns could include things like sudden movements, large crowds gathering in unusual places, or groups of people moving in ways that are inconsistent with normal crowd behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2942bcca",
      "metadata": {
        "id": "2942bcca"
      },
      "source": [
        "## Methodology\n",
        "- Collect Data: The first step in building a crowd anomaly detection model is to collect a large dataset of crowd behavior. This data can be collected using sensors such as cameras, microphones, or other IoT devices.\n",
        "\n",
        "- Preprocess Data: Once the data is collected, it needs to be preprocessed to remove any noise or outliers that may interfere with the analysis. This can involve tasks such as filtering, smoothing, and normalization.\n",
        "\n",
        "- Data Annotation: In order to train a deep learning model, the dataset needs to be labeled with appropriate annotations that indicate normal and abnormal behavior. This can be done manually by human annotators or using automated tools.\n",
        "\n",
        "- Split Data: The labeled dataset is then split into training, validation, and testing sets. The training set is used to train the model, the validation set is used to optimize the hyperparameters of the model, and the testing set is used to evaluate the performance of the model.\n",
        "\n",
        "- Build Model: Once the data is preprocessed and split, the next step is to build a deep learning model. Convolutional Neural Networks (CNN) are commonly used for crowd anomaly detection tasks due to their ability to learn spatial features from images or videos.\n",
        "\n",
        "- Train Model: The model is then trained using the labeled training dataset. This involves repeatedly presenting the model with input data and adjusting the model's parameters to minimize the error between the predicted outputs and the ground truth.\n",
        "\n",
        "- Validate Model: After the model is trained, it is validated on the validation dataset to ensure that it is not overfitting to the training data. This involves monitoring the model's performance on the validation set and adjusting the model's hyperparameters as needed.\n",
        "\n",
        "- Test Model: Finally, the model is tested on the testing dataset to evaluate its performance on new, unseen data. The performance metrics of the model, such as accuracy, precision, recall, F1-score, etc., are computed and analyzed.\n",
        "\n",
        "- Deploy Model: If the model performs well on the testing dataset, it can be deployed in the real world to detect crowd anomalies in real-time. This involves integrating the model with sensors and other IoT devices and developing a user interface for the end-users to visualize and analyze the model's outputs."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fr2hc8WRN5qq"
      },
      "id": "fr2hc8WRN5qq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "82c96878",
      "metadata": {
        "id": "82c96878"
      },
      "source": [
        "## Dataset\n",
        "- [Avenue Dataset](http://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c9fe139",
      "metadata": {
        "id": "1c9fe139"
      },
      "source": [
        "## Papers\n",
        "[Abnormal Event Detection in Videos using Spatiotemporal Autoencoder](https://arxiv.org/abs/1701.01546)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c936abec",
      "metadata": {
        "id": "c936abec"
      },
      "source": [
        "## 1. Load data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnI5Jx5f4XBG",
        "outputId": "6f207280-ab3e-4ccc-cb92-1a2e8bb5ed99"
      },
      "id": "EnI5Jx5f4XBG",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "bzWKvpDkLOGS"
      },
      "id": "bzWKvpDkLOGS",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array,load_img\n",
        "import numpy as np\n",
        "import glob\n",
        "import os \n",
        "from skimage import data, color\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "import argparse\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "train_imagestore=[]\n",
        "\n",
        "\n",
        "\n",
        "train_video_source_path='/content/drive/MyDrive/Zummit/training_videos'\n",
        "fps=5\n",
        "#fps refers to the number of seconds after which one frame will be taken . fps=5 means 1 frame after every 5 seconds. More like seconds per frame.\n",
        "\n",
        "def create_dir(path):\n",
        "\tif not os.path.exists(path):\n",
        "\t\tos.makedirs(path)\n",
        "\n",
        "def remove_old_images(path):\n",
        "\tfilelist = glob.glob(os.path.join(path, \"*.png\"))\n",
        "\tfor f in filelist:\n",
        "\t\tos.remove(f)\n",
        "\n",
        "def store(image_path):\n",
        "\timg=load_img(image_path)\n",
        "\timg=img_to_array(img)\n",
        "\n",
        "\n",
        "\t#Resize the Image to (227,227,3) for the network to be able to process it. \n",
        "\timg=resize(img,(227,227,3))\n",
        "\n",
        "\t#Convert the Image to Grayscale\n",
        "\tgray=0.2989*img[:,:,0]+0.5870*img[:,:,1]+0.1140*img[:,:,2]\n",
        "\n",
        "\ttrain_imagestore.append(gray)\n",
        "\n",
        "\n",
        "\n",
        "#List of all Videos in the Source Directory.\n",
        "videos=os.listdir(train_video_source_path)\n",
        "print(\"Found \",len(videos),\" training videos\")\n",
        "\n",
        "\n",
        "#Make a temp dir to store all the frames\n",
        "create_dir(train_video_source_path+'/frames')\n",
        "\n",
        "#Remove old images\n",
        "remove_old_images(train_video_source_path+'/frames')\n",
        "\n",
        "framepath=train_video_source_path+'/frames'\n",
        "\n",
        "for video in videos:\n",
        "\t\tos.system( 'ffmpeg -i {}/{} -r 1/{}  {}/frames/%03d.jpg'.format(train_video_source_path,video,fps,train_video_source_path))\n",
        "\t\timages=os.listdir(framepath)\n",
        "\t\tfor image in images:\n",
        "\t\t\timage_path=framepath+ '/'+ image\n",
        "\t\t\tstore(image_path)\n",
        "\n",
        "\n",
        "train_imagestore=np.array(train_imagestore)\n",
        "a,b,c=train_imagestore.shape\n",
        "#Reshape to (227,227,batch_size)\n",
        "train_imagestore.resize(b,c,a)\n",
        "#Normalize\n",
        "train_imagestore=(train_imagestore-train_imagestore.mean())/(train_imagestore.std())\n",
        "#Clip negative Values\n",
        "train_imagestore=np.clip(train_imagestore,0,1)\n",
        "np.save('trainer.npy',train_imagestore)\n",
        "#Remove Buffer Directory\n",
        "os.system('rm -r {}'.format(framepath))\n",
        "print(\"Program ended. Please wait while trainer.npy is created. \\nRefresh when needed\")\n",
        "print('Number of frames created :', int(len(train_imagestore)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbnMshkD4Y5G",
        "outputId": "de43ced8-f7a9-4b4c-bb41-3c5ca17f3da8"
      },
      "id": "WbnMshkD4Y5G",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found  16  training videos\n",
            "Program ended. Please wait while trainer.npy is created. \n",
            "Refresh when needed\n",
            "Number of frames created : 227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import numpy as np \n",
        "import argparse\n",
        "from tensorflow.keras.layers import Conv3D,ConvLSTM2D,Conv3DTranspose\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "''' The following load_model function code has been taken from \n",
        "Abnormal Event Detection in Videos using Spatiotemporal Autoencoder\n",
        "by Yong Shean Chong Yong Haur Tay\n",
        "Lee Kong Chian Faculty of Engineering Science, Universiti Tunku Abdul Rahman, 43000 Kajang, Malaysia.\n",
        "It's main purpose is to help us generate the anomaly detector model\n",
        "'''\n",
        "\n",
        "#load_model starts here :----------------------------------------------------\n",
        "def load_model():\n",
        "\t\"\"\"\n",
        "\tReturn the model used for abnormal event \n",
        "\tdetection in videos using spatiotemporal autoencoder\n",
        "\n",
        "\t\"\"\"\n",
        "\tmodel=Sequential()\n",
        "\tmodel.add(Conv3D(filters=128,kernel_size=(11,11,1),strides=(4,4,1),padding='valid',input_shape=(227,227,10,1),activation='tanh'))\n",
        "\tmodel.add(Conv3D(filters=64,kernel_size=(5,5,1),strides=(2,2,1),padding='valid',activation='tanh'))\n",
        "\n",
        "\n",
        "\n",
        "\tmodel.add(ConvLSTM2D(filters=64,kernel_size=(3,3),strides=1,padding='same',dropout=0.4,recurrent_dropout=0.3,return_sequences=True))\n",
        "\n",
        "\t\n",
        "\tmodel.add(ConvLSTM2D(filters=32,kernel_size=(3,3),strides=1,padding='same',dropout=0.3,return_sequences=True))\n",
        "\n",
        "\n",
        "\tmodel.add(ConvLSTM2D(filters=64,kernel_size=(3,3),strides=1,return_sequences=True, padding='same',dropout=0.5))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\tmodel.add(Conv3DTranspose(filters=128,kernel_size=(5,5,1),strides=(2,2,1),padding='valid',activation='tanh'))\n",
        "\tmodel.add(Conv3DTranspose(filters=1,kernel_size=(11,11,1),strides=(4,4,1),padding='valid',activation='tanh'))\n",
        "\n",
        "\tmodel.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n",
        "\n",
        "\treturn model\n",
        "\n",
        "#load_model ends here :----------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "X_train=np.load('trainer.npy')\n",
        "frames=X_train.shape[2]\n",
        "#Need to make number of frames divisible by 10 to ease the load_model\n",
        "\n",
        "\n",
        "frames=frames-frames%10\n",
        "\n",
        "X_train=X_train[:,:,:frames]\n",
        "X_train=X_train.reshape(-1,227,227,10)\n",
        "X_train=np.expand_dims(X_train,axis=4)\n",
        "Y_train=X_train.copy()\n",
        "\n",
        "\n",
        "epochs=100\n",
        "batch_size=1"
      ],
      "metadata": {
        "id": "N7ycIXlw4Y8A"
      },
      "id": "N7ycIXlw4Y8A",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=load_model()\n",
        "\n",
        "callback_save = ModelCheckpoint(\"AnomalyDetector.h5\",\n",
        "                monitor=\"mean_squared_error\")\n",
        "\n",
        "callback_early_stopping = EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "print('Trainer has been loaded')\n",
        "model.fit(X_train,Y_train,\n",
        "      batch_size=batch_size,\n",
        "      epochs=epochs,\n",
        "      callbacks = [callback_save,callback_early_stopping]\n",
        "      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH1r15G04Y-_",
        "outputId": "cb9177a6-9e31-4ee5-f64b-8162e07876e1"
      },
      "id": "MH1r15G04Y-_",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer has been loaded\n",
            "Epoch 1/100\n",
            "20/20 [==============================] - 55s 2s/step - loss: 0.2312 - accuracy: 0.5254\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.2064 - accuracy: 0.5457\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.2020 - accuracy: 0.5458\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.2004 - accuracy: 0.5458\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.1733 - accuracy: 0.5827\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.1244 - accuracy: 0.6707\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.1054 - accuracy: 0.6943\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0961 - accuracy: 0.7044\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0911 - accuracy: 0.7121\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0881 - accuracy: 0.7164\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0858 - accuracy: 0.7178\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 48s 2s/step - loss: 0.0842 - accuracy: 0.7183\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 50s 2s/step - loss: 0.0830 - accuracy: 0.7193\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0819 - accuracy: 0.7199\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0813 - accuracy: 0.7204\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0807 - accuracy: 0.7209\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0810 - accuracy: 0.7213\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0801 - accuracy: 0.7219\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0787 - accuracy: 0.7230\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0776 - accuracy: 0.7246\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0775 - accuracy: 0.7245\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0782 - accuracy: 0.7231\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 48s 2s/step - loss: 0.0773 - accuracy: 0.7245\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0768 - accuracy: 0.7256\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0759 - accuracy: 0.7270\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0756 - accuracy: 0.7275\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0760 - accuracy: 0.7274\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0746 - accuracy: 0.7290\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 47s 2s/step - loss: 0.0742 - accuracy: 0.7290\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0733 - accuracy: 0.7301\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 44s 2s/step - loss: 0.0732 - accuracy: 0.7303\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 44s 2s/step - loss: 0.0730 - accuracy: 0.7306\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 44s 2s/step - loss: 0.0732 - accuracy: 0.7307\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 44s 2s/step - loss: 0.0728 - accuracy: 0.7315\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 44s 2s/step - loss: 0.0724 - accuracy: 0.7316\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0722 - accuracy: 0.7319\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0723 - accuracy: 0.7323\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0716 - accuracy: 0.7324\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0717 - accuracy: 0.7326\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0713 - accuracy: 0.7328\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0714 - accuracy: 0.7330\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0713 - accuracy: 0.7333\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0710 - accuracy: 0.7333\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0709 - accuracy: 0.7334\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 44s 2s/step - loss: 0.0709 - accuracy: 0.7338\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0712 - accuracy: 0.7337\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0711 - accuracy: 0.7339\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0706 - accuracy: 0.7338\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0706 - accuracy: 0.7341\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0704 - accuracy: 0.7341\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0701 - accuracy: 0.7343\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 45s 2s/step - loss: 0.0700 - accuracy: 0.7344\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 44s 2s/step - loss: 0.0700 - accuracy: 0.7345\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 44s 2s/step - loss: 0.0698 - accuracy: 0.7345\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 44s 2s/step - loss: 0.0699 - accuracy: 0.7346\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 44s 2s/step - loss: 0.0698 - accuracy: 0.7346\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 43s 2s/step - loss: 0.0697 - accuracy: 0.7346\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 44s 2s/step - loss: 0.0698 - accuracy: 0.7347\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0698 - accuracy: 0.7347\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0697 - accuracy: 0.7348\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 48s 2s/step - loss: 0.0701 - accuracy: 0.7346\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0698 - accuracy: 0.7347\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0697 - accuracy: 0.7350\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0698 - accuracy: 0.7348\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0697 - accuracy: 0.7350\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 47s 2s/step - loss: 0.0695 - accuracy: 0.7350\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 47s 2s/step - loss: 0.0695 - accuracy: 0.7351\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0694 - accuracy: 0.7352\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0692 - accuracy: 0.7352\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0691 - accuracy: 0.7353\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0694 - accuracy: 0.7352\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 49s 2s/step - loss: 0.0696 - accuracy: 0.7352\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 46s 2s/step - loss: 0.0693 - accuracy: 0.7354\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a138e9a60>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_imagestore=[]\n",
        "\n",
        "\n",
        "\n",
        "test_video_source_path='/content/drive/MyDrive/Zummit/testing_videos'\n",
        "fps=5\n",
        "#fps refers to the number of seconds after which one frame will be taken . fps=5 means 1 frame after every 5 seconds. More like seconds per frame.\n",
        "\n",
        "def create_dir(path):\n",
        "\tif not os.path.exists(path):\n",
        "\t\tos.makedirs(path)\n",
        "\n",
        "def remove_old_images(path):\n",
        "\tfilelist = glob.glob(os.path.join(path, \"*.png\"))\n",
        "\tfor f in filelist:\n",
        "\t\tos.remove(f)\n",
        "\n",
        "def store(image_path):\n",
        "\timg=load_img(image_path)\n",
        "\timg=img_to_array(img)\n",
        "\n",
        "\n",
        "\t#Resize the Image to (227,227,3) for the network to be able to process it. \n",
        "\n",
        "\n",
        "\timg=resize(img,(227,227,3))\n",
        "\n",
        "\t#Convert the Image to Grayscale\n",
        "\n",
        "\n",
        "\tgray=0.2989*img[:,:,0]+0.5870*img[:,:,1]+0.1140*img[:,:,2]\n",
        "\n",
        "\ttest_imagestore.append(gray)\n",
        "#List of all Videos in the Source Directory.\n",
        "videos=os.listdir(test_video_source_path)\n",
        "print(\"Found \",len(videos),\" testing videos\")\n",
        "\n",
        "\n",
        "#Make a temp dir to store all the frames\n",
        "create_dir(test_video_source_path+'/frames')\n",
        "\n",
        "#Remove old images\n",
        "remove_old_images(test_video_source_path+'/frames')\n",
        "\n",
        "framepath=test_video_source_path+'/frames'\n",
        "total=0\n",
        "video_count=0\n",
        "\n",
        "for video in videos:\n",
        "\t\tvideo_count+=1\n",
        "\t\tprint(\"Video number: \",video_count)\n",
        "\t\tprint(\"Video:\",str(video))\n",
        "\t\timage_count=0\n",
        "\t\tos.system( 'ffmpeg -i {}/{} -r 1/{}  {}/frames/%03d.jpg'.format(test_video_source_path,video,fps,test_video_source_path))\n",
        "\t\timages=os.listdir(framepath)\n",
        "\t\timage_count=len(images)\n",
        "\t\tfor image in images:\n",
        "\t\t\timage_path=framepath+ '/'+ image\n",
        "\t\t\tstore(image_path)\n",
        "\t\ttotal=len(images)+total\n",
        "\t\tprint(\"Number of images:\",image_count,\"\\n----------\\n\")\n",
        "\n",
        "\n",
        "test_imagestore=np.array(test_imagestore)\n",
        "a,b,c=test_imagestore.shape\n",
        "#Reshape to (227,227,batch_size)\n",
        "test_imagestore.resize(b,c,a)\n",
        "#Normalize\n",
        "test_imagestore=(test_imagestore-test_imagestore.mean())/(test_imagestore.std())\n",
        "#Clip negative Values\n",
        "test_imagestore=np.clip(test_imagestore,0,1)\n",
        "np.save('tester.npy',test_imagestore)\n",
        "#Remove Buffer Directory\n",
        "os.system('rm -r {}'.format(framepath))\n",
        "\n",
        "print(\"Program ended. All testing videos shall be stored in tester.npy \\n Please wait while tester.npy is created. \\nRefresh when needed\")\n",
        "print('Number of frames created :', int(total))\n",
        "print ('Number of bunches=',int(total),\"/10 = \",int(total/10))\n",
        "print(\"\\nCorrupted and unreadable bunches were ignored\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y0NIZSGHhG7",
        "outputId": "34f8216a-264f-4f51-d2ab-ba518acf4add"
      },
      "id": "-Y0NIZSGHhG7",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found  22  testing videos\n",
            "Video number:  1\n",
            "Video: 13.avi\n",
            "Number of images: 6 \n",
            "----------\n",
            "\n",
            "Video number:  2\n",
            "Video: 21.avi\n",
            "Number of images: 6 \n",
            "----------\n",
            "\n",
            "Video number:  3\n",
            "Video: 16.avi\n",
            "Number of images: 8 \n",
            "----------\n",
            "\n",
            "Video number:  4\n",
            "Video: 20.avi\n",
            "Number of images: 8 \n",
            "----------\n",
            "\n",
            "Video number:  5\n",
            "Video: 19.avi\n",
            "Number of images: 8 \n",
            "----------\n",
            "\n",
            "Video number:  6\n",
            "Video: 06.avi\n",
            "Number of images: 12 \n",
            "----------\n",
            "\n",
            "Video number:  7\n",
            "Video: 10.avi\n",
            "Number of images: 12 \n",
            "----------\n",
            "\n",
            "Video number:  8\n",
            "Video: 04.avi\n",
            "Number of images: 12 \n",
            "----------\n",
            "\n",
            "Video number:  9\n",
            "Video: 08.avi\n",
            "Number of images: 12 \n",
            "----------\n",
            "\n",
            "Video number:  10\n",
            "Video: 18.avi\n",
            "Number of images: 12 \n",
            "----------\n",
            "\n",
            "Video number:  11\n",
            "Video: 05.avi\n",
            "Number of images: 12 \n",
            "----------\n",
            "\n",
            "Video number:  12\n",
            "Video: 17.avi\n",
            "Number of images: 12 \n",
            "----------\n",
            "\n",
            "Video number:  13\n",
            "Video: 02.avi\n",
            "Number of images: 12 \n",
            "----------\n",
            "\n",
            "Video number:  14\n",
            "Video: 09.avi\n",
            "Number of images: 12 \n",
            "----------\n",
            "\n",
            "Video number:  15\n",
            "Video: 03.avi\n",
            "Number of images: 12 \n",
            "----------\n",
            "\n",
            "Video number:  16\n",
            "Video: 07.avi\n",
            "Number of images: 12 \n",
            "----------\n",
            "\n",
            "Video number:  17\n",
            "Video: 11.avi\n",
            "Number of images: 12 \n",
            "----------\n",
            "\n",
            "Video number:  18\n",
            "Video: 14.avi\n",
            "Number of images: 12 \n",
            "----------\n",
            "\n",
            "Video number:  19\n",
            "Video: 15.avi\n",
            "Number of images: 12 \n",
            "----------\n",
            "\n",
            "Video number:  20\n",
            "Video: 12.avi\n",
            "Number of images: 12 \n",
            "----------\n",
            "\n",
            "Video number:  21\n",
            "Video: 01.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  22\n",
            "Video: frames\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Program ended. All testing videos shall be stored in tester.npy \n",
            " Please wait while tester.npy is created. \n",
            "Refresh when needed\n",
            "Number of frames created : 242\n",
            "Number of bunches= 242 /10 =  24\n",
            "\n",
            "Corrupted and unreadable bunches were ignored\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_squared_loss(x1,x2):\n",
        "\n",
        "\n",
        "\t''' Compute Euclidean Distance Loss  between \n",
        "\tinput frame and the reconstructed frame'''\n",
        "\n",
        "\n",
        "\tdiff=x1-x2\n",
        "\ta,b,c,d,e=diff.shape\n",
        "\tn_samples=a*b*c*d*e\n",
        "\tsq_diff=diff**2\n",
        "\tSum=sq_diff.sum()\n",
        "\tdist=np.sqrt(Sum)\n",
        "\tmean_dist=dist/n_samples\n",
        "\n",
        "\treturn mean_dist"
      ],
      "metadata": {
        "id": "KUaG7YOn4ZC9"
      },
      "id": "KUaG7YOn4ZC9",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold=0.00040\n",
        "\n",
        "model=load_model('AnomalyDetector.h5')\n",
        "\n",
        "X_test=np.load('tester.npy')\n",
        "frames=X_test.shape[2]\n",
        "#Need to make number of frames divisible by 10\n",
        "\n",
        "\n",
        "flag=0 #Overall video flagq\n",
        "\n",
        "frames=frames-frames%10\n",
        "\n",
        "X_test=X_test[:,:,:frames]\n",
        "X_test=X_test.reshape(-1,227,227,10)\n",
        "X_test=np.expand_dims(X_test,axis=4)\n",
        "counter =0\n",
        "for number,bunch in enumerate(X_test):\n",
        "\tn_bunch=np.expand_dims(bunch,axis=0)\n",
        "\treconstructed_bunch=model.predict(n_bunch)\n",
        "\n",
        "\n",
        "\tloss=mean_squared_loss(n_bunch,reconstructed_bunch)\n",
        "\t\n",
        "\tif loss>threshold:\n",
        "\t\tprint(\"Anomalous bunch of frames at bunch number {}\".format(number))\n",
        "\t\tcounter=counter+1\n",
        "\t\tprint(\"bunch number: \",counter)\n",
        "\t\tflag=1\n",
        "\n",
        "\n",
        "\telse:\n",
        "\t\tprint('No anomaly')\n",
        "\t\tcounter=counter+1\n",
        "\t\tprint(\"bunch number: \",counter)\n",
        "\n",
        "\n",
        "\n",
        "if flag==1:\n",
        "\tprint(\"Anomalous Events detected\")\n",
        "else:\n",
        "\tprint(\"No anomaly detected\")\n",
        "\t\n",
        "print(\"\\nCorrupted and unreadable bunches were ignored\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Go01rY04ZHe",
        "outputId": "c159343e-f013-495c-e31b-f05135ced207"
      },
      "id": "4Go01rY04ZHe",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 53s 53s/step\n",
            "No anomaly\n",
            "bunch number:  1\n",
            "1/1 [==============================] - 1s 546ms/step\n",
            "No anomaly\n",
            "bunch number:  2\n",
            "1/1 [==============================] - 1s 573ms/step\n",
            "Anomalous bunch of frames at bunch number 2\n",
            "bunch number:  3\n",
            "1/1 [==============================] - 1s 548ms/step\n",
            "Anomalous bunch of frames at bunch number 3\n",
            "bunch number:  4\n",
            "1/1 [==============================] - 1s 539ms/step\n",
            "No anomaly\n",
            "bunch number:  5\n",
            "1/1 [==============================] - 1s 569ms/step\n",
            "No anomaly\n",
            "bunch number:  6\n",
            "1/1 [==============================] - 1s 567ms/step\n",
            "No anomaly\n",
            "bunch number:  7\n",
            "1/1 [==============================] - 1s 546ms/step\n",
            "No anomaly\n",
            "bunch number:  8\n",
            "1/1 [==============================] - 1s 586ms/step\n",
            "No anomaly\n",
            "bunch number:  9\n",
            "1/1 [==============================] - 1s 693ms/step\n",
            "No anomaly\n",
            "bunch number:  10\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "No anomaly\n",
            "bunch number:  11\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "No anomaly\n",
            "bunch number:  12\n",
            "1/1 [==============================] - 1s 918ms/step\n",
            "No anomaly\n",
            "bunch number:  13\n",
            "1/1 [==============================] - 1s 579ms/step\n",
            "No anomaly\n",
            "bunch number:  14\n",
            "1/1 [==============================] - 1s 771ms/step\n",
            "No anomaly\n",
            "bunch number:  15\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "No anomaly\n",
            "bunch number:  16\n",
            "1/1 [==============================] - 1s 548ms/step\n",
            "No anomaly\n",
            "bunch number:  17\n",
            "1/1 [==============================] - 1s 633ms/step\n",
            "No anomaly\n",
            "bunch number:  18\n",
            "1/1 [==============================] - 1s 788ms/step\n",
            "No anomaly\n",
            "bunch number:  19\n",
            "1/1 [==============================] - 1s 720ms/step\n",
            "No anomaly\n",
            "bunch number:  20\n",
            "1/1 [==============================] - 1s 543ms/step\n",
            "No anomaly\n",
            "bunch number:  21\n",
            "1/1 [==============================] - 1s 511ms/step\n",
            "No anomaly\n",
            "bunch number:  22\n",
            "1/1 [==============================] - 1s 542ms/step\n",
            "Anomalous bunch of frames at bunch number 22\n",
            "bunch number:  23\n",
            "1/1 [==============================] - 1s 553ms/step\n",
            "Anomalous bunch of frames at bunch number 23\n",
            "bunch number:  24\n",
            "Anomalous Events detected\n",
            "\n",
            "Corrupted and unreadable bunches were ignored\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}